{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "Suppose we want to apply Newton's method to a function that we know how to evaluate, but don't have code to differentiate.  This is often because it's difficult/error-prone to write or because the interface by which we call it does not support derivatives.  (Commercial packages often fall in this category.)  So we just have a black box function $f(x)$ and wish to approximate its derivative,\n",
    "$$ f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h} .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.297793622041411e-09"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "diff(np.sin, 0.7, 1e-8) - np.cos(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.77456063177317e-08"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = .5\n",
    "diff(np.tan, x, 1e-8) - 1/np.cos(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula works pretty well for the functions above.  It isn't always so nice, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1271.3873432741966),\n",
       " (1e-12, 140.14843387738802),\n",
       " (1e-10, 0.32726590032689273),\n",
       " (1e-08, 19.79343111347407),\n",
       " (1e-06, 1982.7670766180381),\n",
       " (0.0001, 226466.6387994727)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3.14/2\n",
    "[(h, diff(np.tan, x, h) - 1/np.cos(x)**2)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the derivative is ill conditioned for this function\n",
    "$$ \\kappa(f') = |f''| \\frac{|x|}{|f'|} $$\n",
    "but also that our best accuracy is worse than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255.7655915007897 1576948.220797328 1971.5532288895718 3943.1039573124795\n"
     ]
    }
   ],
   "source": [
    "y = 3.14/2;\n",
    "print(\n",
    "    np.tan(y), # f\n",
    "    np.cos(y)**(-2), # f'\n",
    "    np.cos(y)**(-2) * y / np.tan(y), # cond(f)\n",
    "    2*np.cos(y)**(-3)*np.sin(y) * y * np.cos(y)**2, # cond(f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, we see excellent accuracy so long as the size of $h$ is chosen appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.0001),\n",
       " (1e-12, -0.0001),\n",
       " (1e-10, -1.1182158029987482e-05),\n",
       " (1e-08, 8.89005823409637e-09),\n",
       " (1e-06, 8.274037095116864e-12),\n",
       " (0.0001, -9.489531298885641e-12),\n",
       " (0.01, -5.0168102921151377e-11)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e4\n",
    "[(h, diff(np.log, x, h) - 1/x)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking the user to choose a good value of $h$ is a leaky abstraction and unmanageable complexity in many applications.\n",
    "\n",
    "### Automatically choosing a suitable $h$\n",
    "This is one simple and popular choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1.1191038926094873e-05),\n",
       " (1e-12, -1.109830782817004e-09),\n",
       " (1e-10, -1.109830782817004e-09),\n",
       " (1e-08, -8.599665508239422e-12),\n",
       " (1e-06, -5.0162259288282114e-11),\n",
       " (0.0001, -5.000167712764913e-09),\n",
       " (0.01, -4.967408090441846e-07)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff_wp(f, x, eps=1e-8):\n",
    "    \"\"\"Numerical derivative with Walker and Pernice (1998) choice of step\"\"\"\n",
    "    h = eps * (1 + abs(x))\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "x = 1e4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.0007992778373591136),\n",
       " (1e-12, 8.890058234101161e-05),\n",
       " (1e-10, 8.274037099909037e-08),\n",
       " (1e-08, -6.07747097092215e-09),\n",
       " (1e-06, 4.999621836532242e-07),\n",
       " (0.0001, 5.0001667140975314e-05),\n",
       " (0.01, 0.005016708416794913)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0\n",
    "[(eps, diff_wp(np.exp, x, eps) - np.exp(x)) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of these experiments, `eps = 1e-8` (that is, $\\sqrt{\\epsilon_{\\text{machine}}}$) produces good results.\n",
    "This isn't always the case; consider $\\log x$ for small values of $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.11098307828251563),\n",
       " (1e-12, -0.0008599665507063037),\n",
       " (1e-10, -0.005016225928557105),\n",
       " (1e-08, -0.5000167712751136),\n",
       " (1e-06, -49.67408090441677),\n",
       " (0.0001, -3068.721334766654),\n",
       " (0.01, -9538.52419539635)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e-4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is imperfect, leaving some scaling responsibility to the user.\n",
    "It is the default in PETSc's \"matrix-free\" Newton-type solvers; it's cheap and works well when problems are well scaled.\n",
    "It's worth considering why we use\n",
    "$$ h_{WP} = \\sqrt{\\epsilon_{\\text{machine}}} (1 + \\lvert x \\rvert) $$\n",
    "instead of $h_? = \\epsilon_{\\text{machine}} \\lvert x \\rvert$.\n",
    "This choice would be scale-invariant, but problematic when $f(0)$ is not small, as in the example $f(x) = e^x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of numerical differentiation\n",
    "\n",
    "#### Taylor expansion\n",
    "\n",
    "Classical accuracy analysis assumes that functions are sufficiently smooth, meaning that derivatives exist and Taylor expansions are valid within a neighborhood.  In particular,\n",
    "$$ f(x+h) = f(x) + f'(x) h + f''(x) \\frac{h^2}{2!} + \\underbrace{f'''(x) \\frac{h^3}{3!} + \\dotsb}_{O(h^3)} . $$\n",
    "\n",
    "The big-$O$ notation is meant in the limit $h\\to 0$.  Specifically, a function $g(h) \\in O(h^p)$ (sometimes written $g(h) = O(h^p)$) when\n",
    "there exists a constant $C$ such that\n",
    "$$ g(h) \\le C h^p $$\n",
    "for all sufficiently *small* $h$.\n",
    "\n",
    "**Note:** When analyzing algorithms, we will refer to cost being $O(n^2)$, for example, where $n\\to \\infty$.\n",
    "In this case, the above definition applies for sufficiently *large* $n$.  Whether the limit is small ($h\\to 0$) or large ($n \\to\\infty$) should be clear from context.\n",
    "\n",
    "#### Discretization error\n",
    "The `diff` and `diff_wp` functions use a \"forward difference\" formula\n",
    "$$ \\tilde f'(x) := \\frac{f(x+h) - f(x)}{h}.$$\n",
    "Using the Taylor expansion of $f(x+h),$ we compute the discretization error\n",
    "$$ \\begin{split} \\frac{f(x+h) - f(x)}{h} - f'(x) &= \\frac{f(x) + f'(x) h + f''(x) h^2/2 + O(h^3) - f(x)}{h} - f'(x) \\\\\n",
    "&= \\frac{f'(x) h + f''(x) h^2/2 + O(h^3)}{h} - f'(x) \\\\\n",
    "&= f''(x) h/2 + O(h^2) .\n",
    "\\end{split} $$\n",
    "\n",
    "This is the *discretization error* caused by choosing a finite (not infinitesimal) differencing parameter $h$, and the leading order term depends linearly on $h$.\n",
    "This is also called *truncation error*, due to truncating the Taylor series; the terms are interchangeable.\n",
    "\n",
    "#### Rounding error\n",
    "We have an additional source of error, *rounding error*, which comes from not being able to compute $f(x)$ or $f(x+h)$ exactly, nor subtract them exactly.  Suppose that we can, however, compute these functions with a relative error on the order of $\\epsilon_{\\text{machine}}$.  This leads to\n",
    "$$ \\begin{split}\n",
    "\\tilde f(x) &= f(x)(1 + \\epsilon_1) \\\\\n",
    "\\tilde f(x \\oplus h) &= \\tilde f((x+h)(1 + \\epsilon_2)) \\\\\n",
    "&= f((x + h)(1 + \\epsilon_2))(1 + \\epsilon_3) \\\\\n",
    "&= [f(x+h) + f'(x+h)(x+h)\\epsilon_2 + O(\\epsilon_2^2)](1 + \\epsilon_3) \\\\\n",
    "&= f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where each $\\epsilon_i$ is an independent relative error on the order of $\\epsilon_{\\text{machine}}$ and we have used a Taylor expansion at $x+h$ to approximate $f(x \\oplus h)$.\n",
    "We thus write the rounding error in the forward difference approximation as\n",
    "$$ \\begin{split}\n",
    "\\left\\lvert \\frac{\\tilde f(x+h) \\ominus \\tilde f(x)}{h} - \\frac{f(x+h) - f(x)}{h} \\right\\rvert &=\n",
    "  \\left\\lvert \\frac{f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h) - f(x)(1 + \\epsilon_1) - f(x+h) + f(x)}{h} \\right\\rvert \\\\\n",
    "  &\\le \\frac{|f(x+h)\\epsilon_3| + |f'(x+h)x\\epsilon_2| + |f(x)\\epsilon_1| + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}}h)}{h} \\\\\n",
    "  &\\le \\frac{(2 \\max_{[x,x+h]} |f| + \\max_{[x,x+h]} |f' x| \\epsilon_{\\text{machine}} + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)}{h} \\\\\n",
    "  &= (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} + O(\\epsilon_{\\text{machine}}) \\\\\n",
    "\\end{split} $$\n",
    "where we have assumed that $h \\ge \\epsilon_{\\text{machine}}$.\n",
    "This error becomes large (relative to $f'$ -- we are concerned with relative error after all)\n",
    "* $f$ is large compared to $f'$\n",
    "* $x$ is large\n",
    "* $h$ is too small\n",
    "\n",
    "#### Total error and optimal $h$\n",
    "\n",
    "Suppose we would like to choose $h$ to minimize the combined discretization and rounding error,\n",
    "$$ h^* = \\arg\\min_h | f''(x) h/2 | + (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} $$\n",
    "(dropping the higher order terms), which we can compute by differentiating with respect to $h$ and setting the result equal to zero\n",
    "$$ |f''|/2 - (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h^2} = 0 $$\n",
    "which can be rearranged as\n",
    "$$ h^* = \\sqrt{\\frac{4\\max|f| + 2\\max|f'x|}{|f''|}} \\sqrt{\\epsilon_{\\text{machine}}} .$$\n",
    "Of course this formula is of little use for computing $h$ because all this is to compute $f'$, which we obviously don't know yet, much less $f''$.\n",
    "However, it does have value:\n",
    "* It explains why `1e-8` (i.e., $\\sqrt{\\epsilon_{\\text{machine}}}$) was empirically found to be about optimal for well-behaved/scaled functions.\n",
    "* It explains why even for the best behaved functions, our best attainable accuracy with forward differencing is $\\sqrt{\\epsilon_{\\text{machine}}}$.\n",
    "* If we have some special knowledge about the class of functions we need to differentiate, we might have bounds on these quantities and thus an ability to use this formula to improve accuracy.  Alternatively, we could run a parameter sweep to empirically choose a suitable $h$, though we would have to re-tune in response to parameter changes in the class of functions.\n",
    "* If someone claims to have a simple and robust rule for computing $h$ then this formula tells us how to build a function that breaks their rule.  There are no silver bullets.\n",
    "* If our numerical differentiation routine produces a poor approximation for some function that we run into in the wild, this helps us explain what happened and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered difference\n",
    "\n",
    "Instead of the forward difference approximation\n",
    "$$ \\frac{f(x+h) - f(x)}{h} $$\n",
    "we could use the centered difference formula,\n",
    "$$ \\frac{f(x+h) - f(x-h)}{2h} . $$\n",
    "(One way to derive this formula is to average a forward and backward difference.  We will learn a more general method later in the course when we do interpolation.)\n",
    "We can compute the discretization error by Taylor expansion,\n",
    "\\begin{split} \\frac{f(x) + f'(x)h + f''(x)h^2/2 + f'''(x)h^3/6 - f(x) + f'(x)h - f''(x)h^2/2 + f'''(x) h^3/6 + O(h^4)}{2h} \\\\\n",
    "= f'(x) + f'''(x)h^2/6 + O(h^3) \\end{split}\n",
    "showing that the leading error term is of order $h^2$, versus order $h$ for forward differences.\n",
    "A similar computation including rounding error will find that the optimal $h$ is now of order $\\sqrt[3]{\\epsilon_{\\text{machine}}}$ so the best attainable accuracy is $\\epsilon_{\\text{machine}}^{2/3}$.\n",
    "This accuracy improvement (versus $\\sqrt{\\epsilon_{\\text{machine}}}$) is significant, but we'll also see that it is twice as expensive when computing derivatives of multi-variate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic differentiation\n",
    "\n",
    "We've been differentiating basic mathematical functions, for which there is a formula for the derivative.\n",
    "Symbolic differentiation is a tool that can compute those expressions (and generate code to evaluate the expressions numerically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}$"
      ],
      "text/plain": [
       "log(x)*cos(x**pi)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy.abc import x\n",
    "\n",
    "f = sympy.cos(x**sympy.pi) * sympy.log(x)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}$"
      ],
      "text/plain": [
       "-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y = log(x)*cos(pow(x, M_PI));'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.ccode(f, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      y = log(x)*cos(x**3.1415926535897932d0)'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.fcode(f, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.215513413838041906745231945917755720873$"
      ],
      "text/plain": [
       "0.2155134138380419067452319459177557208730"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.evalf(40, subs={x: 1.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}$"
      ],
      "text/plain": [
       "log(log(x)*cos(x**pi))*cos((log(x)*cos(x**pi))**pi)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x, m=np):\n",
    "    y = x\n",
    "    for i in range(2):\n",
    "        y = m.cos(y**m.pi) * m.log(y)\n",
    "    return y\n",
    "\n",
    "gexpr = g(x, m=sympy)\n",
    "gexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{\\pi \\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\sin{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}} + \\frac{\\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}}$"
      ],
      "text/plain": [
       "-pi*(log(x)*cos(x**pi))**pi*(-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*log(log(x)*cos(x**pi))*sin((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi)) + (-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*cos((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi))"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.diff(gexpr, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-coding derivatives\n",
    "\n",
    "The size of these expressions grow exponentially in the number of loop iterations, yet one can write efficient code for computing the derivative by hand.  We use the variational notation\n",
    "\n",
    "$$ \\operatorname{d} f = f'(x) \\operatorname{d} x $$\n",
    "\n",
    "which allows us to break a large computation into simple pieces that we can compute incrementally, instead of trying to build up expressions for complicated functions.  That is, we can differentiate a composition $h(g(f(x)))$ as\n",
    "\n",
    "\\begin{align}\n",
    "  \\operatorname{d} h &= h' \\operatorname{d} g \\\\\n",
    "  \\operatorname{d} g &= g' \\operatorname{d} f \\\\\n",
    "  \\operatorname{d} f &= f' \\operatorname{d} x.\n",
    "\\end{align}\n",
    "Consider our example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by hand (-1.5346823414986814, -34.03241959914048)\n",
      "numerical -34.032439961925064\n"
     ]
    }
   ],
   "source": [
    "def gprime(x):\n",
    "    y = x\n",
    "    dy = 1\n",
    "    for i in range(2):\n",
    "        a = np.log(y)\n",
    "        da = 1/y * dy\n",
    "        b = y ** np.pi\n",
    "        db = np.pi * y ** (np.pi - 1) * dy\n",
    "        c = np.cos(b)\n",
    "        dc = -np.sin(b) * db\n",
    "        y = c * a\n",
    "        dy = dc * a + c * da\n",
    "    return y, dy\n",
    "\n",
    "print('by hand', gprime(1.9))\n",
    "print('numerical', diff_wp(g, 1.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic (automatic) differentiation\n",
    "\n",
    "Next, we'll consider ways to have libraries/compilers generate by-hand code such as we see above."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

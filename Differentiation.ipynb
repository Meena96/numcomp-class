{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numerical differentiation\n",
    "\n",
    "Suppose we want to apply Newton's method to a function that we know how to evaluate, but don't have code to differentiate.  This is often because it's difficult/error-prone to write or because the interface by which we call it does not support derivatives.  (Commercial packages often fall in this category.)  So we just have a black box function $f(x)$ and wish to approximate its derivative,\n",
    "$$ f'(x) = \\lim_{h\\to 0} \\frac{f(x+h) - f(x)}{h} .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.297793622041411e-09"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff(f, x, h=1e-5):\n",
    "    return (f(x + h) - f(x)) / h\n",
    "\n",
    "diff(np.sin, 0.7, 1e-8) - np.cos(0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.77456063177317e-08"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = .5\n",
    "diff(np.tan, x, 1e-8) - 1/np.cos(x)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This formula works pretty well for the functions above.  It isn't always so nice, however:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1271.3873432741966),\n",
       " (1e-12, 140.14843387738802),\n",
       " (1e-10, 0.32726590032689273),\n",
       " (1e-08, 19.79343111347407),\n",
       " (1e-06, 1982.7670766180381),\n",
       " (0.0001, 226466.6387994727)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 3.14/2\n",
    "[(h, diff(np.tan, x, h) - 1/np.cos(x)**2)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the derivative is ill conditioned for this function\n",
    "$$ \\kappa(f') = |f''| \\frac{|x|}{|f'|} $$\n",
    "but also that our best accuracy is worse than "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1255.7655915007897 1576948.220797328 1971.5532288895718 3943.1039573124795\n"
     ]
    }
   ],
   "source": [
    "y = 3.14/2;\n",
    "print(\n",
    "    np.tan(y), # f\n",
    "    np.cos(y)**(-2), # f'\n",
    "    np.cos(y)**(-2) * y / np.tan(y), # cond(f)\n",
    "    2*np.cos(y)**(-3)*np.sin(y) * y * np.cos(y)**2, # cond(f')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In other cases, we see excellent accuracy so long as the size of $h$ is chosen appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.0001),\n",
       " (1e-12, -0.0001),\n",
       " (1e-10, -1.1182158029987482e-05),\n",
       " (1e-08, 8.89005823409637e-09),\n",
       " (1e-06, 8.274037095116864e-12),\n",
       " (0.0001, -9.489531298885641e-12),\n",
       " (0.01, -5.0168102921151377e-11)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e4\n",
    "[(h, diff(np.log, x, h) - 1/x)\n",
    " for h in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asking the user to choose a good value of $h$ is a leaky abstraction and unmanageable complexity in many applications.\n",
    "\n",
    "### Automatically choosing a suitable $h$\n",
    "This is one simple and popular choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -1.1191038926094873e-05),\n",
       " (1e-12, -1.109830782817004e-09),\n",
       " (1e-10, -1.109830782817004e-09),\n",
       " (1e-08, -8.599665508239422e-12),\n",
       " (1e-06, -5.0162259288282114e-11),\n",
       " (0.0001, -5.000167712764913e-09),\n",
       " (0.01, -4.967408090441846e-07)]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def diff_wp(f, x, eps=1e-8):\n",
    "    \"\"\"Numerical derivative with Walker and Pernice (1998) choice of step\"\"\"\n",
    "    h = eps * (1 + abs(x))\n",
    "    return (f(x+h) - f(x)) / h\n",
    "\n",
    "x = 1e4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.0007992778373591136),\n",
       " (1e-12, 8.890058234101161e-05),\n",
       " (1e-10, 8.274037099909037e-08),\n",
       " (1e-08, -6.07747097092215e-09),\n",
       " (1e-06, 4.999621836532242e-07),\n",
       " (0.0001, 5.0001667140975314e-05),\n",
       " (0.01, 0.005016708416794913)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 0\n",
    "[(eps, diff_wp(np.exp, x, eps) - np.exp(x)) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In both of these experiments, `eps = 1e-8` (that is, $\\sqrt{\\epsilon_{\\text{machine}}}$) produces good results.\n",
    "This isn't always the case; consider $\\log x$ for small values of $x$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1e-14, -0.11098307828251563),\n",
       " (1e-12, -0.0008599665507063037),\n",
       " (1e-10, -0.005016225928557105),\n",
       " (1e-08, -0.5000167712751136),\n",
       " (1e-06, -49.67408090441677),\n",
       " (0.0001, -3068.721334766654),\n",
       " (0.01, -9538.52419539635)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = 1e-4\n",
    "[(eps, diff_wp(np.log, x, eps) - 1/x) for eps in [1e-14, 1e-12, 1e-10, 1e-8, 1e-6, 1e-4, 1e-2]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This algorithm is imperfect, leaving some scaling responsibility to the user.\n",
    "It is the default in PETSc's \"matrix-free\" Newton-type solvers; it's cheap and works well when problems are well scaled.\n",
    "It's worth considering why we use\n",
    "$$ h_{WP} = \\sqrt{\\epsilon_{\\text{machine}}} (1 + \\lvert x \\rvert) $$\n",
    "instead of $h_? = \\epsilon_{\\text{machine}} \\lvert x \\rvert$.\n",
    "This choice would be scale-invariant, but problematic when $f(0)$ is not small, as in the example $f(x) = e^x$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of numerical differentiation\n",
    "\n",
    "#### Taylor expansion\n",
    "\n",
    "Classical accuracy analysis assumes that functions are sufficiently smooth, meaning that derivatives exist and Taylor expansions are valid within a neighborhood.  In particular,\n",
    "$$ f(x+h) = f(x) + f'(x) h + f''(x) \\frac{h^2}{2!} + \\underbrace{f'''(x) \\frac{h^3}{3!} + \\dotsb}_{O(h^3)} . $$\n",
    "\n",
    "The big-$O$ notation is meant in the limit $h\\to 0$.  Specifically, a function $g(h) \\in O(h^p)$ (sometimes written $g(h) = O(h^p)$) when\n",
    "there exists a constant $C$ such that\n",
    "$$ g(h) \\le C h^p $$\n",
    "for all sufficiently *small* $h$.\n",
    "\n",
    "**Note:** When analyzing algorithms, we will refer to cost being $O(n^2)$, for example, where $n\\to \\infty$.\n",
    "In this case, the above definition applies for sufficiently *large* $n$.  Whether the limit is small ($h\\to 0$) or large ($n \\to\\infty$) should be clear from context.\n",
    "\n",
    "#### Discretization error\n",
    "The `diff` and `diff_wp` functions use a \"forward difference\" formula\n",
    "$$ \\tilde f'(x) := \\frac{f(x+h) - f(x)}{h}.$$\n",
    "Using the Taylor expansion of $f(x+h),$ we compute the discretization error\n",
    "$$ \\begin{split} \\frac{f(x+h) - f(x)}{h} - f'(x) &= \\frac{f(x) + f'(x) h + f''(x) h^2/2 + O(h^3) - f(x)}{h} - f'(x) \\\\\n",
    "&= \\frac{f'(x) h + f''(x) h^2/2 + O(h^3)}{h} - f'(x) \\\\\n",
    "&= f''(x) h/2 + O(h^2) .\n",
    "\\end{split} $$\n",
    "\n",
    "This is the *discretization error* caused by choosing a finite (not infinitesimal) differencing parameter $h$, and the leading order term depends linearly on $h$.\n",
    "This is also called *truncation error*, due to truncating the Taylor series; the terms are interchangeable.\n",
    "\n",
    "#### Rounding error\n",
    "We have an additional source of error, *rounding error*, which comes from not being able to compute $f(x)$ or $f(x+h)$ exactly, nor subtract them exactly.  Suppose that we can, however, compute these functions with a relative error on the order of $\\epsilon_{\\text{machine}}$.  This leads to\n",
    "$$ \\begin{split}\n",
    "\\tilde f(x) &= f(x)(1 + \\epsilon_1) \\\\\n",
    "\\tilde f(x \\oplus h) &= \\tilde f((x+h)(1 + \\epsilon_2)) \\\\\n",
    "&= f((x + h)(1 + \\epsilon_2))(1 + \\epsilon_3) \\\\\n",
    "&= [f(x+h) + f'(x+h)(x+h)\\epsilon_2 + O(\\epsilon_2^2)](1 + \\epsilon_3) \\\\\n",
    "&= f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)\n",
    "\\end{split}\n",
    "$$\n",
    "\n",
    "where each $\\epsilon_i$ is an independent relative error on the order of $\\epsilon_{\\text{machine}}$ and we have used a Taylor expansion at $x+h$ to approximate $f(x \\oplus h)$.\n",
    "We thus write the rounding error in the forward difference approximation as\n",
    "$$ \\begin{split}\n",
    "\\left\\lvert \\frac{\\tilde f(x+h) \\ominus \\tilde f(x)}{h} - \\frac{f(x+h) - f(x)}{h} \\right\\rvert &=\n",
    "  \\left\\lvert \\frac{f(x+h)(1 + \\epsilon_3) + f'(x+h)x\\epsilon_2 + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h) - f(x)(1 + \\epsilon_1) - f(x+h) + f(x)}{h} \\right\\rvert \\\\\n",
    "  &\\le \\frac{|f(x+h)\\epsilon_3| + |f'(x+h)x\\epsilon_2| + |f(x)\\epsilon_1| + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}}h)}{h} \\\\\n",
    "  &\\le \\frac{(2 \\max_{[x,x+h]} |f| + \\max_{[x,x+h]} |f' x| \\epsilon_{\\text{machine}} + O(\\epsilon_{\\text{machine}}^2 + \\epsilon_{\\text{machine}} h)}{h} \\\\\n",
    "  &= (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} + O(\\epsilon_{\\text{machine}}) \\\\\n",
    "\\end{split} $$\n",
    "where we have assumed that $h \\ge \\epsilon_{\\text{machine}}$.\n",
    "This error becomes large (relative to $f'$ -- we are concerned with relative error after all)\n",
    "* $f$ is large compared to $f'$\n",
    "* $x$ is large\n",
    "* $h$ is too small\n",
    "\n",
    "#### Total error and optimal $h$\n",
    "\n",
    "Suppose we would like to choose $h$ to minimize the combined discretization and rounding error,\n",
    "$$ h^* = \\arg\\min_h | f''(x) h/2 | + (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h} $$\n",
    "(dropping the higher order terms), which we can compute by differentiating with respect to $h$ and setting the result equal to zero\n",
    "$$ |f''|/2 - (2\\max|f| + \\max|f'x|) \\frac{\\epsilon_{\\text{machine}}}{h^2} = 0 $$\n",
    "which can be rearranged as\n",
    "$$ h^* = \\sqrt{\\frac{4\\max|f| + 2\\max|f'x|}{|f''|}} \\sqrt{\\epsilon_{\\text{machine}}} .$$\n",
    "Of course this formula is of little use for computing $h$ because all this is to compute $f'$, which we obviously don't know yet, much less $f''$.\n",
    "However, it does have value:\n",
    "* It explains why `1e-8` (i.e., $\\sqrt{\\epsilon_{\\text{machine}}}$) was empirically found to be about optimal for well-behaved/scaled functions.\n",
    "* It explains why even for the best behaved functions, our best attainable accuracy with forward differencing is $\\sqrt{\\epsilon_{\\text{machine}}}$.\n",
    "* If we have some special knowledge about the class of functions we need to differentiate, we might have bounds on these quantities and thus an ability to use this formula to improve accuracy.  Alternatively, we could run a parameter sweep to empirically choose a suitable $h$, though we would have to re-tune in response to parameter changes in the class of functions.\n",
    "* If someone claims to have a simple and robust rule for computing $h$ then this formula tells us how to build a function that breaks their rule.  There are no silver bullets.\n",
    "* If our numerical differentiation routine produces a poor approximation for some function that we run into in the wild, this helps us explain what happened and how to fix it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centered difference\n",
    "\n",
    "Instead of the forward difference approximation\n",
    "$$ \\frac{f(x+h) - f(x)}{h} $$\n",
    "we could use the centered difference formula,\n",
    "$$ \\frac{f(x+h) - f(x-h)}{2h} . $$\n",
    "(One way to derive this formula is to average a forward and backward difference.  We will learn a more general method later in the course when we do interpolation.)\n",
    "We can compute the discretization error by Taylor expansion,\n",
    "\\begin{split} \\frac{f(x) + f'(x)h + f''(x)h^2/2 + f'''(x)h^3/6 - f(x) + f'(x)h - f''(x)h^2/2 + f'''(x) h^3/6 + O(h^4)}{2h} \\\\\n",
    "= f'(x) + f'''(x)h^2/6 + O(h^3) \\end{split}\n",
    "showing that the leading error term is of order $h^2$, versus order $h$ for forward differences.\n",
    "A similar computation including rounding error will find that the optimal $h$ is now of order $\\sqrt[3]{\\epsilon_{\\text{machine}}}$ so the best attainable accuracy is $\\epsilon_{\\text{machine}}^{2/3}$.\n",
    "This accuracy improvement (versus $\\sqrt{\\epsilon_{\\text{machine}}}$) is significant, but we'll also see that it is twice as expensive when computing derivatives of multi-variate functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolic differentiation\n",
    "\n",
    "We've been differentiating basic mathematical functions, for which there is a formula for the derivative.\n",
    "Symbolic differentiation is a tool that can compute those expressions (and generate code to evaluate the expressions numerically)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}$"
      ],
      "text/plain": [
       "log(x)*cos(x**pi)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sympy\n",
    "from sympy.abc import x\n",
    "\n",
    "f = sympy.cos(x**sympy.pi) * sympy.log(x)\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}$"
      ],
      "text/plain": [
       "-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.diff(f, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'y = log(x)*cos(pow(x, M_PI));'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.ccode(f, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'      y = log(x)*cos(x**3.1415926535897932d0)'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.fcode(f, 'y')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle 0.215513413838041906745231945917755720873$"
      ],
      "text/plain": [
       "0.2155134138380419067452319459177557208730"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.evalf(40, subs={x: 1.9})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}$"
      ],
      "text/plain": [
       "log(log(x)*cos(x**pi))*cos((log(x)*cos(x**pi))**pi)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def g(x, m=np):\n",
    "    y = x\n",
    "    for i in range(2):\n",
    "        # a = m.log(y)\n",
    "        # b = y ** m.pi\n",
    "        # c = m.cos(b)\n",
    "        # y = c * a\n",
    "        y = m.cos(y**m.pi) * m.log(y)\n",
    "    return y\n",
    "\n",
    "gexpr = g(x, m=sympy)\n",
    "gexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$\\displaystyle - \\frac{\\pi \\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\log{\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)} \\right)} \\sin{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}} + \\frac{\\left(- \\frac{\\pi x^{\\pi} \\log{\\left(x \\right)} \\sin{\\left(x^{\\pi} \\right)}}{x} + \\frac{\\cos{\\left(x^{\\pi} \\right)}}{x}\\right) \\cos{\\left(\\left(\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}\\right)^{\\pi} \\right)}}{\\log{\\left(x \\right)} \\cos{\\left(x^{\\pi} \\right)}}$"
      ],
      "text/plain": [
       "-pi*(log(x)*cos(x**pi))**pi*(-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*log(log(x)*cos(x**pi))*sin((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi)) + (-pi*x**pi*log(x)*sin(x**pi)/x + cos(x**pi)/x)*cos((log(x)*cos(x**pi))**pi)/(log(x)*cos(x**pi))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sympy.diff(gexpr, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hand-coding derivatives\n",
    "\n",
    "The size of these expressions grow exponentially in the number of loop iterations, yet one can write efficient code for computing the derivative by hand.  We use the variational notation\n",
    "\n",
    "$$ \\operatorname{d} f = f'(x) \\operatorname{d} x $$\n",
    "\n",
    "which allows us to break a large computation into simple pieces that we can compute incrementally, instead of trying to build up expressions for complicated functions.  That is, we can differentiate a composition $h(g(f(x)))$ as\n",
    "\n",
    "\\begin{align}\n",
    "  \\operatorname{d} h &= h' \\operatorname{d} g \\\\\n",
    "  \\operatorname{d} g &= g' \\operatorname{d} f \\\\\n",
    "  \\operatorname{d} f &= f' \\operatorname{d} x.\n",
    "\\end{align}\n",
    "Consider our example above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "by hand (-1.5346823414986814, -34.03241959914048)\n",
      "numerical -34.032439961925064\n"
     ]
    }
   ],
   "source": [
    "def gprime(x):\n",
    "    y = x\n",
    "    dy = 1\n",
    "    for i in range(2):\n",
    "        a = np.log(y)\n",
    "        da = 1/y * dy\n",
    "        b = y ** np.pi\n",
    "        db = np.pi * y ** (np.pi - 1) * dy\n",
    "        c = np.cos(b)\n",
    "        dc = -np.sin(b) * db\n",
    "        y = c * a\n",
    "        dy = dc * a + c * da\n",
    "    return y, dy\n",
    "\n",
    "print('by hand', gprime(1.9))\n",
    "print('numerical', diff_wp(g, 1.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This code is pretty mechanical to write\n",
    "* It's hard to maintain as you add new features\n",
    "* It's hard to debug\n",
    "  * You can test using finite differencing\n",
    "  * You can take apart pieces for unit testing and/or debugging\n",
    "* If you know you'll be writing this sort of code, plan ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational notation is handy (an example)\n",
    "\n",
    "We'll differentiate the expression\n",
    "\n",
    "$$ I = A^{-1} A $$\n",
    "applying the product rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 0 = A^{-1} (\\operatorname dA) + (\\operatorname dA^{-1}) A $$\n",
    "and collect terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\operatorname dA^{-1} = - A^{-1} (\\operatorname dA) A^{-1}. $$\n",
    "\n",
    "This expression for the derivative $\\operatorname d A^{-1}$ in direction $\\operatorname d A$ is useful when differentiating algorithmn that involve linear algebra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reverse-mode\n",
    "\n",
    "What we've done above is called \"forward mode\", and amounts to placing the parentheses in the chain rule like\n",
    "\n",
    "$$ \\operatorname d h = \\frac{dh}{dg} \\left(\\frac{dg}{df} \\left(\\frac{df}{dx} \\operatorname d x \\right) \\right) .$$\n",
    "\n",
    "The expression means the same thing if we rearrange the parentheses,\n",
    "\n",
    "$$ \\operatorname d h = \\left( \\left( \\left( \\frac{dh}{dg} \\right) \\frac{dg}{df} \\right) \\frac{df}{dx} \\right) \\operatorname d x .$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward (-1.5346823414986814, -34.03241959914048)\n",
      "reverse (-1.5346823414986814, -34.03241959914049)\n"
     ]
    }
   ],
   "source": [
    "def gprime_rev(x):\n",
    "    # First compute all the values by going through the iteration forwards\n",
    "    # I'm unrolling two iterations here for clarity (\"static single assignment\" form)\n",
    "    # It is possible to write code that keeps the loop structure.\n",
    "    a1 = np.log(x)\n",
    "    b1 = x ** np.pi\n",
    "    c1 = np.cos(b1)\n",
    "    y1 = c1 * a1\n",
    "    a2 = np.log(y1)\n",
    "    b2 = y1 ** np.pi\n",
    "    c2 = np.cos(b2)\n",
    "    y = c2 * a2 # Result\n",
    "    # Now go backwards computing dy/d_ for each variable\n",
    "    y_ = 1\n",
    "    y_c2 = y_ * a2\n",
    "    y_a2 = c2 * y_\n",
    "    y_b2 = -y_c2 * np.sin(b2) # dy/db2 = dy/dc2 dc2/db2\n",
    "    y_y1 = y_b2 * np.pi * y1 ** (np.pi - 1) + y_a2 / y1\n",
    "    y_c1 = y_y1 * a1\n",
    "    y_a1 = c1 * y_y1\n",
    "    y_b1 = -y_c1 * np.sin(b1)\n",
    "    y_x = y_b1 * np.pi * x ** (np.pi - 1) + y_a1 / x\n",
    "    return y, y_x\n",
    "\n",
    "print('forward', gprime(1.9))\n",
    "print('reverse', gprime_rev(1.9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This is fairly mechanical, similar to forward-mode\n",
    "* It is more complicated than forward-mode\n",
    "* This sort of code is tricky to debug\n",
    "  * You can test using forward-mode or finite differencing\n",
    "* We need the results of intermediate computation in reverse order\n",
    "  * We have to store those values somewhere (\"taping\" in the literature)\n",
    "  * Or we have to recompute them (see \"hierarchical checkpointing\")\n",
    "* Reverse-mode is also known as the \"adjoint\" method and \"back-propagation\".\n",
    "  \n",
    "### Why reverse?\n",
    "\n",
    "If all we had was scalar functions of scalar inputs, we would never use reverse mode.  But let's suppose we are given a dot product with a constant vector.\n",
    "\n",
    "$$ f(\\mathbf x) = \\mathbf c^T \\mathbf x = \\begin{pmatrix} c_0 & c_1 & c_2 & \\dotsb \\end{pmatrix} \\begin{pmatrix} x_0 \\\\ x_1 \\\\ x_2 \\\\ \\vdots \\end{pmatrix} $$\n",
    "and wish to compute the gradient\n",
    "$$ \\nabla_{\\mathbf x} f = \\frac{\\partial f}{\\partial \\mathbf x} = \\begin{pmatrix} \\frac{\\partial f}{\\partial x_0} & \\frac{\\partial f}{\\partial x_1} & \\frac{\\partial f}{\\partial x_2} & \\dotsb \\end{pmatrix} . $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.7881902865938712"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot(c, x):\n",
    "    n = len(c)\n",
    "    sum = 0\n",
    "    for i in range(n):\n",
    "        sum += c[i] * x[i]\n",
    "    return sum\n",
    "        \n",
    "n = 20\n",
    "c = np.random.randn(n)\n",
    "x = np.random.randn(n)\n",
    "f = dot(c, x)\n",
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use forward mode, we can only compute one direction at a time, effectively\n",
    "$$ \\left(\\nabla_{\\mathbf x} f\\right) \\cdot \\operatorname d x $$\n",
    "for one value of the vector $\\operatorname d x$ at a time.\n",
    "We can compute the full gradient by choosing $\\operatorname d x$ to be each column of the identity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41288309, -0.02618   , -1.63423663,  0.3006944 , -0.25078029,\n",
       "       -0.45382014,  1.94208405,  0.30730437,  0.45064477, -0.89218614,\n",
       "       -1.25475455, -0.76033123, -0.54544687, -1.40029722, -0.74489052,\n",
       "       -0.18788402, -1.00248867,  1.5845576 , -0.29049147,  0.98277278])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dot_x(c, x, dx):\n",
    "    \"\"\"Compute derivative in direction dx\"\"\"\n",
    "    n = len(c)\n",
    "    dsum = 0\n",
    "    for i in range(n):\n",
    "        dsum += c[i] * dx[i]\n",
    "    return dsum\n",
    "\n",
    "def grad_dot(c, x):\n",
    "    n = len(c)\n",
    "    I = np.eye(n)\n",
    "    grad = np.zeros(n)\n",
    "    for j in range(n):\n",
    "        dx = I[:,j]\n",
    "        grad[j] = dot_x(c, x, dx)\n",
    "    return grad\n",
    "\n",
    "grad_dot(c, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've now traversed the loop with our work as many times as there are components in the vector.  The forward evaluation for `dot` costs $O(n)$ and computing the gradient costs $O(n^2)$ because we have to do $O(n)$ for for each direction and there are $n$ directions.\n",
    "\n",
    "Compare with reverse-mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41288309, -0.02618   , -1.63423663,  0.3006944 , -0.25078029,\n",
       "       -0.45382014,  1.94208405,  0.30730437,  0.45064477, -0.89218614,\n",
       "       -1.25475455, -0.76033123, -0.54544687, -1.40029722, -0.74489052,\n",
       "       -0.18788402, -1.00248867,  1.5845576 , -0.29049147,  0.98277278])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def grad_dot_rev(c, x):\n",
    "    n = len(c)\n",
    "    sum_ = np.zeros(n)\n",
    "    for i in range(n):\n",
    "        sum_[i] = c[i]\n",
    "    return sum_\n",
    "\n",
    "grad_dot_rev(c, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We get the same values in only $O(n)$ work!\n",
    "* The astute reader may recall that we already worked out this case,\n",
    "$$ \\frac{\\partial \\mathbf c^T \\mathbf x}{\\partial \\mathbf x} = \\mathbf c^T .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shape of the gradient (Jacobian)\n",
    "\n",
    "Suppose we have a vector-valued function of vector-valued input, $\\mathbf f(\\mathbf x)$ where $\\mathbf f$ has length $m$ and $\\mathbf x$ has length $n$.\n",
    "* The gradient (Jacobian) matrix $J = \\nabla_{\\mathbf x} \\mathbf f$ has shape $m\\times n$.\n",
    "* Usually in optimization, $m=1$ because we only have one objective\n",
    "* If $m\\ll n$ then finite differencing and forward-mode differentiation will be much more expensive than reverse-mode differentiation\n",
    "  * Find a way to use reverse-mode!\n",
    "* If $m \\approx n$ then either is about as efficient, but forward-mode is simpler.\n",
    "* If $m \\gg n$ then forward-mode is the ticket.\n",
    "* In real computations, there may be expensive stages that have lower dimension inputs or outputs, in which case those can be captured. An example is\n",
    "$$ \\mathbf f(\\mathbf x) = \\mathbf q \\sigma(\\mathbf q^T \\mathbf x) $$\n",
    "where $\\sigma$ is an expensive nonlinear function.\n",
    "The Jacobian $J = \\nabla_{\\mathbf x} \\mathbf f$ is a square matrix, but naive forward- and reverse-mode would both require $n$ evaluations of $\\sigma$.\n",
    "Since $\\sigma$ is a scalar-valued function of a scalar argument, $\\sigma'(\\mathbf q^T \\mathbf x)$ is just one number, and thus $J = (\\sigma') \\mathbf q \\mathbf q^T$ is readily available (and you know it's rank-1 so don't need to store all $n^2$ entries). Models of this sort show up frequently in physical modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algorithmic (automatic) differentiation\n",
    "\n",
    "Next, we'll consider ways to have libraries/compilers generate by-hand code such as we see above.\n",
    "We'll use the [JAX](https://jax.readthedocs.io/en/latest/) library, which offers differentiation of NumPy computations (and offload to GPUs, which we won't use now).\n",
    "Uncomment the line below if you need to install `jax` and `jaxlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install jax jaxlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-34.03244\n",
      "-34.03241959914048\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "\n",
    "def g_jax(x):\n",
    "    \"\"\"Same function as before, but using jnp in place of np.\"\"\"\n",
    "    y = x\n",
    "    for i in range(2):\n",
    "        y = jnp.cos(y**jnp.pi) * jnp.log(y)\n",
    "    return y\n",
    "\n",
    "gprime_jax = jax.grad(g_jax)\n",
    "print(gprime_jax(1.9))\n",
    "print(gprime(1.9)[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(DeviceArray([-1.9289212 ,  2.097858  ,  0.20503117,  0.22631234,\n",
       "              -1.2956989 ,  0.5156321 , -0.5992178 ,  0.56441456,\n",
       "              -0.23191658,  0.07927939,  0.451759  ,  1.7861412 ,\n",
       "              -1.1517502 ,  0.4325543 ,  0.26122522, -0.44151178,\n",
       "              -1.85014   ,  0.8240289 ,  0.69433916,  0.46808633],            dtype=float32),\n",
       " array([-1.92892122,  2.09785787,  0.20503117,  0.22631233, -1.29569892,\n",
       "         0.51563212, -0.59921775,  0.56441458, -0.23191657,  0.07927939,\n",
       "         0.451759  ,  1.78614113, -1.15175023,  0.4325543 ,  0.26122522,\n",
       "        -0.44151178, -1.85014002,  0.82402888,  0.69433915,  0.46808634]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(dot)(c, x), x # Differentiates with respect to argument zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeviceArray([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "             0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.grad(dot, argnums=1)(c, x) - c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Software\n",
    "\n",
    "* Algorithmic differentiation (AD) software has been around for over 40 years\n",
    "* There are two classical approaches\n",
    "  * Source transformation: AD tool emits Fortran (or C, etc.) code, which is compiled by a normal compiler\n",
    "  * Operator overleading: each basic operation is overloaded to transform objects holding values + derivatives\n",
    "* Source transformation is usually more efficient, retaining loop structure, etc.\n",
    "* Implementations tend to have poor ergonomics, odd restrictions on use, poor composition.\n",
    "* Vectorization has been poor with most classical tools.\n",
    "* AD *implementations* have come a long way in the past few years (despite the math being old)\n",
    "* Just-in-time compilation and extensive software engineering\n",
    "* Exemplars:\n",
    "  * [JAX](https://jax.readthedocs.io/en/latest/) for Python\n",
    "  * [Zygote.jl](http://fluxml.ai/Zygote.jl/latest/) for Julia\n",
    "* AD is great within its domain, but is still intrusive (especially for multi-language projects, languages with poor AD tooling, etc.).  Even in JAX, you'll see [various constraints](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html), such as that you can't in-place update an array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<class 'jax.interpreters.xla.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-2a7f31191bcf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/.local/lib/python3.8/site-packages/jax/numpy/lax_numpy.py\u001b[0m in \u001b[0;36m_unimplemented_setitem\u001b[0;34m(self, i, x)\u001b[0m\n\u001b[1;32m   3394\u001b[0m          \u001b[0;34m\"immutable; perhaps you want jax.ops.index_update or \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3395\u001b[0m          \"jax.ops.index_add instead?\")\n\u001b[0;32m-> 3396\u001b[0;31m   \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3398\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_operator_round\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumber\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mndigits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: '<class 'jax.interpreters.xla.DeviceArray'>' object does not support item assignment. JAX arrays are immutable; perhaps you want jax.ops.index_update or jax.ops.index_add instead?"
     ]
    }
   ],
   "source": [
    "z = jnp.zeros(3)\n",
    "z[1] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* If you work in this space, you'll eventually learn to judge when to use AD and when to hand-code a derivative.  This type of decision lies at the intersection of numerical analysis and software engineering."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
